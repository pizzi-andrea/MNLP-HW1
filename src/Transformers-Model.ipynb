{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d33d8a",
   "metadata": {},
   "source": [
    "# Cultural Classification with Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed66b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r drive/MyDrive/AIRO\\ \\S2/MNLP/MNLP_Homework1/mirror/ .\n",
    "%cd mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73fa24",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to load dataset \n",
    "from CU_Dataset_Factory import Hf_Loader, Local_Loader, CU_Dataset_Factory\n",
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "from datasets import Dataset\n",
    "from datasets import Features\n",
    "from datasets import Split, Value\n",
    "from transformers import EarlyStoppingCallback\n",
    "from time import time\n",
    "\n",
    "# Imports for Transformers\n",
    "from transformers import AutoTokenizer # Datasets\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification # Model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import numpy as np # Evaluation\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "from torch.nn import Module\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ae2a3",
   "metadata": {},
   "source": [
    "## Global Notebook Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b04ccf",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0303aff",
   "metadata": {},
   "source": [
    "Choose appropriate features. Available features are:\n",
    "\n",
    "* *'description'* - synthetic Wikidata description\n",
    "* *'intro'* - Wikipedia page introduction\n",
    "* *'full_page*' - full Wikipedia plain-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92860eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65edf8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = 'intro'\n",
    "\n",
    "train_file      = \"train.csv\" #@param {type:\"string\"}\n",
    "validation_file = \"validation.csv\" #@param {type:\"string\"}\n",
    "\n",
    "######################################################\n",
    "# don't modify this row for testing purposes #\n",
    "test_file       = \"tr_test.tsv\" #@param {type:\"string\"} #\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192117a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = CU_Dataset_Factory(f'./experiment_n{time()}')\n",
    "if is_train:\n",
    "    train_l = Hf_Loader(\"sapienzanlp/nlp2025_hw1_cultural_dataset\", 'train')\n",
    "    validation_l = Hf_Loader(\"sapienzanlp/nlp2025_hw1_cultural_dataset\", 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    train = factory.produce(train_l, 'tr_train.tsv', [fe], 'label', 10, False, False)\n",
    "    validation  = factory.produce(validation_l, 'tr_validation.tsv', [fe], 'label', 10,False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    train_data = train[['label', fe]].rename({fe: 'text'}, axis=1)\n",
    "    validation_data = validation[['label', fe]].rename({fe: 'text'}, axis=1)\n",
    "\n",
    "    # Prepare Dataset for the Model\n",
    "\n",
    "    train_data = Dataset.from_pandas(train_data, features=Features({\n",
    "        'label': Value('int32'),\n",
    "        'text' : Value('string')\n",
    "    }), split=Split.TRAIN)\n",
    "\n",
    "    validation_data = Dataset.from_pandas(validation_data, features=Features({\n",
    "        'label': Value('int32'),\n",
    "        'text' : Value('string')\n",
    "    }), split=Split.VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee2523",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "    def process_samples(self, samples):\n",
    "        return samples.map(lambda sample: self.tokenizer(sample['text'], truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48789e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "# may customize the classification head after import\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "p = Preprocessor(tokenizer)\n",
    "\n",
    "if is_train:\n",
    "    tokenize_train = p.process_samples(train_data)\n",
    "    tokenize_validation = p.process_samples(validation_data)\n",
    "    \n",
    "collector = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fcedb",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f906880",
   "metadata": {},
   "source": [
    "### Tested Models\n",
    "We have tested major pretrained model using different features, for each one we have reported accuracy value\n",
    "* google/mobilebert-uncased (wiki_desc  - 72%)\n",
    "* microsoft/deberta-v3-xsmall (wiki_desc - 78%)\n",
    "* distilbert/distilbert-base-uncased-finetuned-sst-2-english (wiki_desc - 75%)\n",
    "* microsoft/MiniLM-L12-H384-uncased\n",
    "* distilbert/distilbert-base-uncased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_repo, num_labels=3, ignore_mismatched_sizes=True)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased-distilled-squad', num_labels=3, ignore_mismatched_sizes=True)\n",
    "    pass\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f362c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = evaluate.load(\"accuracy\")\n",
    "   load_f1 = evaluate.load(\"f1\")\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels, average='micro')[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba19fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CU_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, config:dict[str, int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(config['dim_embedding'], config['hidden_layers'], bias=True),\n",
    "            nn.LayerNorm(config['hidden_layers']),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(config['hidden_layers'], config['hidden_layers'], bias=True),\n",
    "            nn.LayerNorm(config['hidden_layers']),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(config['hidden_layers'], config['dim_embedding'], bias=True),\n",
    "            nn.LayerNorm(config['dim_embedding']),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(in_features=config['dim_embedding'], out_features=config['num_classes'])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.l1(X)\n",
    "        X = self.out(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "config = {\n",
    "    'dim_embedding' : 768,\n",
    "    'hidden_layers' : 900,\n",
    "    'num_classes'   : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = CU_Classifier(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d524a12",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063df6b3",
   "metadata": {},
   "source": [
    "### Training Phase (enabled if `is_train` is True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "epochs = 5\n",
    "batch_size = 4\n",
    "weight_decay = 1e-4\n",
    "learning_rate = 2e-5\n",
    "out_dir = 'CU_with_DBert'\n",
    "log = 'Cultural Analysis'\n",
    "\n",
    "cls2label = {0:'Cultural Agnostic', 1:'Cultural Rapresentative', 2:'Cultural Exclusive'}\n",
    "label2cls = {l:c for c ,l in cls2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    eval_strategy='epoch',\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=learning_rate,             \n",
    "    report_to=\"none\",\n",
    "    logging_dir=log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    trainer = Trainer(model,traning_args, collector, tokenize_train, tokenize_validation,tokenizer,compute_metrics=compute_metrics)\n",
    "    print(f'Model running on {trainer.model.device}')\n",
    "    trainer.train()\n",
    "    report = trainer.evaluate()\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242e5c7",
   "metadata": {},
   "source": [
    "### Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_culture_pd(ds:Dataset, model:Module, tokenizer, device, max_length=128) -> Series:\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    encoding =   ds.map( lambda v: tokenizer(v['text'], return_tensors='pt', max_length=max_length, padding='max_length', truncation=True))\n",
    "    input_ids = tensor( encoding['input_ids'] ).squeeze().to(device)\n",
    "    attention_mask = tensor(encoding['attention_mask']).squeeze().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "    labels = preds.numpy(force=True)\n",
    "    return Series(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Local_Loader(test_file)\n",
    "test = loader.get()\n",
    "#test = factory.produce(loader, out_file=None, enable_feature=[fe], targe_feature=None, batch_s=45)\n",
    "train_data = test[[fe]].rename({fe: 'text'}, axis=1)\n",
    "train_ds = Dataset.from_pandas(train_data, features=Features({\n",
    "    'text' : Value('string')\n",
    "}), split=Split.TEST)\n",
    "\n",
    "# Converts to tensors\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe046bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_culture_pd(train_ds, model, tokenizer, ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "test.insert(loc=len(test.columns), column='label', value=y_pred)\n",
    "\n",
    "print(test[['item', 'name','label']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb31def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Save file for evaluation purposes #\n",
    "#####################################\n",
    "\n",
    "test[['item', 'name','label']].to_csv('Many_Naps_Little_Progress_modello2.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
