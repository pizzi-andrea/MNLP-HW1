{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Base libs for read and show data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn \n",
    "import pathlib as path\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc08ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# const values\n",
    "BASE_DIR = path.PosixPath('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "dataset_t = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset')['train'].to_pandas()\n",
    "dataset_v = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset')['validation'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some dataset entry\n",
    "dataset_t.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf49988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot values distribution to observer particular carachteristics\n",
    "# test dataset\n",
    "category_dist_t = pd.Series.value_counts((dataset_t['category']))\n",
    "type_dist_t = pd.Series.value_counts((dataset_t['type']))\n",
    "sub_dist_t = pd.Series.value_counts((dataset_t['subcategory']))\n",
    "label_dist_t = pd.Series.value_counts((dataset_t['label']))\n",
    "# validation dataset\n",
    "category_dist_v = pd.Series.value_counts((dataset_v['category']))\n",
    "type_dist_v = pd.Series.value_counts((dataset_v['type']))\n",
    "sub_dist_v = pd.Series.value_counts((dataset_v['subcategory']))\n",
    "label_dist_v = pd.Series.value_counts((dataset_v['label']))\n",
    "\n",
    "print(label_dist_t)\n",
    "\n",
    "# Gaussian distribution\n",
    "print(sub_dist_t)\n",
    "\n",
    "# unbalanced\n",
    "print(type_dist_t)\n",
    "\n",
    "# category\n",
    "print(category_dist_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf58a05",
   "metadata": {},
   "source": [
    "## Category Analysis\n",
    "\n",
    "Look up to exploit some information on category, analyzing statistical aspects such as data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39187024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots category distribution\n",
    "plt.figure(figsize=(30,12))\n",
    "plt.title('Category Distribution')\n",
    "plt.xlabel('category')\n",
    "plt.ylabel('num of occurance')\n",
    "plt.bar(category_dist_t.index, category_dist_t, 0.5, label='Train')\n",
    "\n",
    "plt.bar(category_dist_v.index, category_dist_v, 0.5, label='Test')\n",
    "plt.legend(loc='upper left', ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5daba",
   "metadata": {},
   "source": [
    "## Subcategory Analysis\n",
    "\n",
    "Look up to exploit some information on category analyzing statistical aspects, such as data distribution and visual rappresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad955d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots category distribution\n",
    "\n",
    "plt.figure(figsize=(14,20))\n",
    "plt.title('Subcategory Distribution')\n",
    "plt.ylabel('Subcategory')\n",
    "plt.xlabel('Num of Occurance')\n",
    "plt.barh(sub_dist_t.index, sub_dist_t, 0.5, label='Train')\n",
    "plt.barh(sub_dist_v.index, sub_dist_v, 0.5, label='Test')\n",
    "plt.legend(loc='upper left', ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e642a",
   "metadata": {},
   "source": [
    "## Labels Analysis\n",
    "\n",
    "Check the number of classes to identify, this analysis is useful to find unbalanced data factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots category distribution\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Labels Distribution')\n",
    "plt.ylabel('Labels')\n",
    "plt.xlabel('Num of Occurance')\n",
    "plt.barh(label_dist_t.index, label_dist_t, label='Train')\n",
    "plt.barh(label_dist_v.index, label_dist_v, label='Test')\n",
    "plt.legend(loc='upper left', ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d031e",
   "metadata": {},
   "source": [
    "## Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45156f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots category distribution\n",
    "plt.figure(figsize=(30,12))\n",
    "plt.title('Category Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Num of Occurance')\n",
    "plt.bar(type_dist_t.index, type_dist_t, 0.5, label='Train')\n",
    "plt.bar(type_dist_v.index, type_dist_v, 0.5, label='Test')\n",
    "plt.legend(loc='upper left', ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681f352",
   "metadata": {},
   "source": [
    "## Languages Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f033a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ccc0c",
   "metadata": {},
   "source": [
    "## Links Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0191623",
   "metadata": {},
   "source": [
    "## Hyperlinks Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def BFS_Links(G: nx.DiGraph, base: str, limit: int = 50, depth: int = 5) -> nx.DiGraph:\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&titles={base}&prop=links&pllimit=max&format=json\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error: {http_err}\")\n",
    "        return G\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timed out.\")\n",
    "        return G\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error: {err}\")\n",
    "        return G\n",
    "    except ValueError as json_err:\n",
    "        print(f\"JSON parsing error: {json_err}\")\n",
    "        return G\n",
    "\n",
    "    # Add the base node if it's not already in the graph\n",
    "    if not G.has_node(base):\n",
    "        print(f'Added node: {base}')\n",
    "        G.add_node(base, count=0)\n",
    "\n",
    "    if depth == 0:\n",
    "        return G\n",
    "\n",
    "    # Extract links from the API response\n",
    "    links = []\n",
    "    pages = data['query']['pages']\n",
    "    for page_id in pages:\n",
    "        page = pages[page_id]\n",
    "        if \"links\" in page:\n",
    "            links = [link['title'] for link in page['links'][:min(limit, len(page['links']))]]\n",
    "\n",
    "    if not links:\n",
    "        return G\n",
    "\n",
    "    # Explore the links\n",
    "    for link in links:\n",
    "        if G.has_node(link):\n",
    "            G.nodes[link]['count'] += 1  # already present → increment counter\n",
    "        else:\n",
    "            G.add_node(link, count=1)  # new node → count as first visit\n",
    "\n",
    "        G.add_edge(base, link)  # add edge between base and link\n",
    "\n",
    "        print(f\"Node '{link}' has been seen {G.nodes[link]['count']} times.\")\n",
    "        G = BFS_Links(G, link, limit, depth - 1)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Example starting point\n",
    "start_page = \"Human\"\n",
    "url = f\"https://en.wikipedia.org/w/api.php?action=query&titles={start_page}&prop=links&pllimit=max&format=json\"\n",
    "# Add the starting page to the graph\n",
    "# Recall the BFS_Links function\n",
    "G = BFS_Links(G, start_page, limit=5, depth=3)\n",
    "\n",
    "# Now we have a graph with nodes and edges representing Wikipedia pages and links between them\n",
    "\n",
    "# Example analysis: Draw the graph\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G,pos=pos, with_labels=True, node_size=50, node_color=\"skyblue\", font_size=5, font_weight=\"bold\")\n",
    "plt.title(f\"Graph of Wikipedia Links for '{start_page}'\")\n",
    "plt.show()\n",
    "\n",
    "# PageRank analysis (calculating importance of nodes)\n",
    "pagerank = nx.pagerank(G)\n",
    "print(\"PageRank of nodes:\", pagerank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
