{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5df55e1",
   "metadata": {},
   "source": [
    "## The CrossTree\n",
    "\n",
    "Voting Schema with multiple binary classification trees. The network implements a voting scheme based on three different trees:\n",
    "\n",
    "* Cultural **Agnostic-Rappresentative** tree\n",
    "* Cultural **Agnostic-Exclusive** tree\n",
    "* Cultural **Exclusive-Rappresentative** tree\n",
    "\n",
    "the most voted class will be the predicted class.\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "The training process is quite standard and straight-forward: given the n G_features we want to directly predict the associated class.\n",
    "\n",
    "### Employment Phase\n",
    "\n",
    "The training model will be inserted in a wider model called X and utilized as a function for the computation of the G_Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b724d1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afebd71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/MNLP/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from CU_Dataset_Factory import Hf_Loader, CU_Dataset_Factory\n",
    "\n",
    "def onehot_encode(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    cat_cols: list[str]|None = None,\n",
    "    num_cols: list[str]|None = None,\n",
    "    sparse: bool = False\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, OneHotEncoder]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding to df_train and df_test guaranteeing the same\n",
    "    set of columns, even if train is missing categories who are in the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training DataFrame.\n",
    "    df_test : pd.DataFrame\n",
    "        Testing DataFrame.\n",
    "    cat_cols : list[str], optional\n",
    "        List of categorical columns to encode.\n",
    "        If None, all columns of type 'object' are taken.\n",
    "    num_cols : list[str], optional\n",
    "        List of numerical (or non-categorical) columns to preserve.\n",
    "        If None, all columns not in cat_cols are taken. \n",
    "    handle_unknown : str, default=\"ignore\"\n",
    "        Beahavior on unknown values in test (typically \"ignore\").\n",
    "    sparse : bool, default=False\n",
    "        If True, returns sparse matrix, otherwise dense.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_train_enc : pd.DataFrame\n",
    "        Training DataFrame with One-Hot Encoding + original num_cols.\n",
    "    df_test_enc : pd.DataFrame\n",
    "        Testing DataFrame with One-Hot Encoding + original num_cols.\n",
    "    encoder : OneHotEncoder\n",
    "        The fitted OneHotEncoder object, useful for future transform.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Identify category and numerical columns (if not given)\n",
    "    if cat_cols is None:\n",
    "        cat_cols = df_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "    if num_cols is None:\n",
    "        num_cols = [c for c in df_train.columns if c not in cat_cols]\n",
    "\n",
    "    # 2) Fit encoder on all category data (train + test)\n",
    "    all_cats = pd.concat([df_train[cat_cols], df_test[cat_cols]], \n",
    "                         axis=0, ignore_index=True)\n",
    "    encoder = OneHotEncoder(\n",
    "        sparse_output=sparse\n",
    "    ).fit(all_cats)\n",
    "\n",
    "    # 3) Transform separatly train and test\n",
    "    X_train_ohe = encoder.transform(df_train[cat_cols])\n",
    "    X_test_ohe  = encoder.transform(df_test[cat_cols])\n",
    "\n",
    "    # 4) Name the new columns\n",
    "    ohe_cols = encoder.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "    # 5) Compose the final DataFrames\n",
    "    df_train_enc = pd.DataFrame(\n",
    "        np.hstack([X_train_ohe.toarray() if sparse else X_train_ohe,\n",
    "                   df_train[num_cols].values]), # type: ignore\n",
    "        columns=ohe_cols + num_cols,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    df_test_enc = pd.DataFrame(\n",
    "        np.hstack([X_test_ohe.toarray() if sparse else X_test_ohe,\n",
    "                   df_test[num_cols].values]),\n",
    "        columns=ohe_cols + num_cols,\n",
    "        index=df_test.index\n",
    "    )\n",
    "\n",
    "    return df_train_enc[ohe_cols], df_test_enc[ohe_cols], encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5133b41",
   "metadata": {},
   "source": [
    "## Produce the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75c3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cultural Dataset argumentation start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 136.84it/s]\n",
      "copy dataset: 100%|██████████| 10/10 [00:00<00:00, 1335.72it/s]\n",
      "ambiguos:   0%|          | 0/6251 [00:07<?, ?it/s, batch=1]     /home/andrea/Documenti/MNLP-HW1/src/CU_Dataset_Factory.py:218: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.0 0.0 0.0 ... 0.0 0.0 0.0]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  prc_result.loc[:, feature] = prc_result[feature].astype('float64').add(delta, fill_value=0)\n",
      "G:   3%|▎         | 176/6251 [03:36<3:10:16,  1.88s/it, batch=12]         "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m train_l = Hf_Loader(\u001b[33m\"\u001b[39m\u001b[33msapienzanlp/nlp2025_hw1_cultural_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m validation_l = Hf_Loader(\u001b[33m\"\u001b[39m\u001b[33msapienzanlp/nlp2025_hw1_cultural_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain.tsv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msubcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlanguages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreference\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_langs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mambiguos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mG\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_mod\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mback_links\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m factory.produce(validation_l, \u001b[33m'\u001b[39m\u001b[33mvalidation.tsv\u001b[39m\u001b[33m'\u001b[39m, [\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msubcategory\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlanguages\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mnum_langs\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mambiguos\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn_mod\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mback_links\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m16\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEnd process\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/MNLP-HW1/src/CU_Dataset_Factory.py:244\u001b[39m, in \u001b[36mCU_Dataset_Factory.produce\u001b[39m\u001b[34m(self, loader, out_file, enable_feature, targe_feature, batch_s, encoding)\u001b[39m\n\u001b[32m    242\u001b[39m dataset = dataset.drop([\u001b[33m'\u001b[39m\u001b[33mitem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[32m1\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Function that calls back __produce and returns the new dataset\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m prc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__produce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarge_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m prc.to_csv(\u001b[38;5;28mself\u001b[39m.out_dir.joinpath(out_file), sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/MNLP-HW1/src/CU_Dataset_Factory.py:178\u001b[39m, in \u001b[36mCU_Dataset_Factory.__produce\u001b[39m\u001b[34m(self, dataset, enable_feature, targe_feature, batch_s, encode)\u001b[39m\n\u001b[32m    176\u001b[39m join_fe = \u001b[33m'\u001b[39m\u001b[33mwiki_name\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    177\u001b[39m mask = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.sgf)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m r = \u001b[43mG_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mjoin_fe\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m mask:\n\u001b[32m    182\u001b[39m     d = r.set_index(join_fe)[c].to_dict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/MNLP-HW1/src/features.py:266\u001b[39m, in \u001b[36mG_factor\u001b[39m\u001b[34m(titles, qids, limit, depth, max_nodes, time_limit, threads)\u001b[39m\n\u001b[32m    263\u001b[39m     r[col] = \u001b[32m0.0\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     G = \u001b[43mBFS2_Links_Parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHTTP error for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/MNLP-HW1/src/utils.py:480\u001b[39m, in \u001b[36mBFS2_Links_Parallel\u001b[39m\u001b[34m(qid, limit, max_depth, max_nodes, max_runtime, max_concurrent)\u001b[39m\n\u001b[32m    478\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m    479\u001b[39m         nest_asyncio.apply()\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_BFS_Links_Async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_runtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m    484\u001b[39m     \u001b[38;5;66;03m# No active loop: we can also use asyncio.run normally\u001b[39;00m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio.run(\n\u001b[32m    486\u001b[39m         _BFS_Links_Async(qid,limit, max_depth, max_nodes, max_runtime, max_concurrent)\n\u001b[32m    487\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/selectors.py:452\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    450\u001b[39m ready = []\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print('Cultural Dataset argumentation start')\n",
    "factory = CU_Dataset_Factory('.')\n",
    "train_l = Hf_Loader(\"sapienzanlp/nlp2025_hw1_cultural_dataset\", 'train')\n",
    "validation_l = Hf_Loader(\"sapienzanlp/nlp2025_hw1_cultural_dataset\", 'validation')\n",
    "\n",
    "factory.produce(train_l, 'train.tsv', ['category', 'subcategory','type','languages','reference','num_langs', 'ambiguos', 'G', 'n_mod', 'back_links'], 'label', 16, False)\n",
    "factory.produce(validation_l, 'validation.tsv', ['category', 'subcategory','type','languages','reference','num_langs', 'ambiguos', 'G', 'n_mod', 'back_links'], 'label', 16, False)\n",
    "print('End process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "validation = pd.read_csv('validation.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f56e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe997e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[['label']]\n",
    "y_validation = validation[['label']]\n",
    "\n",
    "id_train = train[['wiki_name']]\n",
    "id_validation = validation[['wiki_name']]\n",
    "\n",
    "fe_train = train[['languages', 'num_langs', 'reference']]\n",
    "fe_validation = validation[['languages', 'num_langs', 'reference']]\n",
    "\n",
    "fe_str_train = train[['category', 'subcategory', 'type']]\n",
    "fe_str_validation = validation[['category', 'subcategory', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat, validation_cat, _ =  onehot_encode(fe_str_train, fe_str_validation, ['category'] )\n",
    "train_scat, validation_scat, _ = onehot_encode(fe_str_train, fe_str_validation, ['subcategory'] )\n",
    "train_t, validation_t, _ = onehot_encode(fe_str_train, fe_str_validation, ['type'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_cat.shape)\n",
    "print(validation_scat.shape)\n",
    "print(validation_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fba437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c105c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# validation_cat = pd.DataFrame(pca.fit_transform(validation_cat))\n",
    "# train_cat = pd.DataFrame(pca.fit_transform(train_cat))\n",
    "# pca = PCA(n_components=50)\n",
    "# validation_scat = pd.DataFrame(pca.fit_transform(validation_scat))\n",
    "# train_scat = pd.DataFrame(pca.fit_transform(train_scat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f384973",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_cat.shape)\n",
    "print(validation_scat.shape)\n",
    "print(validation_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([fe_train, train_cat, train_scat, train_t, y_train], axis=1)\n",
    "validation = pd.concat([fe_validation, validation_cat, validation_scat, validation_t, y_validation], axis=1) \n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a46a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ad7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0b35a",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea974e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44b664",
   "metadata": {},
   "source": [
    "### Agnostic-Representative classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train.query(\"label == 0 or label == 1\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_tree = MLPClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation.query(\"label == 0 or label == 1\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeccb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ar_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db137e02",
   "metadata": {},
   "source": [
    "## Agnostic-Exclusive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train.query(\"label == 0 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f024d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tree = MLPClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation.query(\"label == 0 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ae_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660edf4",
   "metadata": {},
   "source": [
    "## Representative-Exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train.query(\"label == 1 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tree = RandomForestClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation.query(\"label == 1 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204616b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = re_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255d629",
   "metadata": {},
   "source": [
    "# Voting Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Cultural-classification Network\n",
    "class CABNet(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, ar_estimator, ae_estimator, re_estimator) -> None:\n",
    "        self.ar = ar_estimator \n",
    "        self.ae = ae_estimator \n",
    "        self.re = re_estimator\n",
    "        \n",
    "    \n",
    "    def __fit(self, e, X, y) -> BaseEstimator:\n",
    "        return e.fit(X, y)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        d = X.query(\"label == 0 or label == 1 \")\n",
    "        y = d['label'].astype(int).to_numpy()\n",
    "        x = d.drop(['label'], axis=1).astype(float).to_numpy()\n",
    "        self.ar = self.__fit(self.ar, x, y)\n",
    "        d = X.query(\"label == 0 or label == 2\")\n",
    "        y = d['label'].astype(int).to_numpy()\n",
    "        x = d.drop(['label'], axis=1).astype(float).to_numpy()\n",
    "        self.ae = self.__fit(self.ae, x, y)\n",
    "        d = X.query(\"label == 1 or label == 2\")\n",
    "        y = d['label'].astype(int).to_numpy()\n",
    "        x = d.drop(['label'], axis=1).astype(float).to_numpy()\n",
    "        self.re = self.__fit(self.re, x, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        v1 = self.ar.predict(X)\n",
    "        v2 = self.ae.predict(X)\n",
    "        v3 = self.re.predict(X)\n",
    "  \n",
    "        votes = np.vstack([v1, v2, v3])\n",
    "        majority, _ = mode(votes, axis=0)\n",
    "        return majority.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, IsolationForest\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd526559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  CABNet(RandomForestClassifier(n_estimators=100), RandomForestClassifier(n_estimators=500), RandomForestClassifier(n_estimators=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d654b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
