{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5df55e1",
   "metadata": {},
   "source": [
    "## The CrossTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f47ce",
   "metadata": {},
   "source": [
    "Voting Schema with multiple binary classification trees. The network implements a voting scheme based on three different trees:\n",
    "\n",
    "* Cultural **Agnostic-Rappresentative** tree\n",
    "* Cultural **Agnostic-Exclusive** tree\n",
    "* Cultural **Exclusive-Rappresentative** tree\n",
    "\n",
    "the most voted class will be the predicted class.\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "The training process is quite standard and straight-forward: given the n G_features we want to directly predict the associated class.\n",
    "\n",
    "### Employment Phase\n",
    "\n",
    "The training model will be inserted in a wider model called X and utilized as a function for the computation of the G_Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848def6d",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from CU_Dataset_Factory import Hf_Loader, CU_Dataset_Factory\n",
    "\n",
    "from sklearn.feature_selection import SelectFdr, chi2, VarianceThreshold, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b724d1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbfbb5",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    cat_cols: list[str]|None = None,\n",
    "    num_cols: list[str]|None = None,\n",
    "    sparse: bool = False\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, OneHotEncoder]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding to df_train and df_test guaranteeing the same\n",
    "    set of columns, even if train is missing categories who are in the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training DataFrame.\n",
    "    df_test : pd.DataFrame\n",
    "        Testing DataFrame.\n",
    "    cat_cols : list[str], optional\n",
    "        List of categorical columns to encode.\n",
    "        If None, all columns of type 'object' are taken.\n",
    "    num_cols : list[str], optional\n",
    "        List of numerical (or non-categorical) columns to preserve.\n",
    "        If None, all columns not in cat_cols are taken. \n",
    "    handle_unknown : str, default=\"ignore\"\n",
    "        Beahavior on unknown values in test (typically \"ignore\").\n",
    "    sparse : bool, default=False\n",
    "        If True, returns sparse matrix, otherwise dense.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_train_enc : pd.DataFrame\n",
    "        Training DataFrame with One-Hot Encoding + original num_cols.\n",
    "    df_test_enc : pd.DataFrame\n",
    "        Testing DataFrame with One-Hot Encoding + original num_cols.\n",
    "    encoder : OneHotEncoder\n",
    "        The fitted OneHotEncoder object, useful for future transform.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Identify category and numerical columns (if not given)\n",
    "    if cat_cols is None:\n",
    "        cat_cols = df_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "    if num_cols is None:\n",
    "        num_cols = [c for c in df_train.columns if c not in cat_cols]\n",
    "\n",
    "    # 2) Fit encoder on all category data (train + test)\n",
    "    all_cats = pd.concat([df_train[cat_cols], df_test[cat_cols]], \n",
    "                         axis=0, ignore_index=True)\n",
    "    encoder = OneHotEncoder(\n",
    "        sparse_output=sparse\n",
    "    ).fit(all_cats)\n",
    "\n",
    "    # 3) Transform separatly train and test\n",
    "    X_train_ohe = encoder.transform(df_train[cat_cols])\n",
    "    X_test_ohe  = encoder.transform(df_test[cat_cols])\n",
    "\n",
    "    # 4) Name the new columns\n",
    "    ohe_cols = encoder.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "    # 5) Compose the final DataFrames\n",
    "    df_train_enc = pd.DataFrame(\n",
    "        np.hstack([X_train_ohe.toarray() if sparse else X_train_ohe,\n",
    "                   df_train[num_cols].values]), # type: ignore\n",
    "        columns=ohe_cols + num_cols,\n",
    "        index=df_train.index\n",
    "    )\n",
    "    df_test_enc = pd.DataFrame(\n",
    "        np.hstack([X_test_ohe.toarray() if sparse else X_test_ohe,\n",
    "                   df_test[num_cols].values]),\n",
    "        columns=ohe_cols + num_cols,\n",
    "        index=df_test.index\n",
    "    )\n",
    "\n",
    "    return df_train_enc[ohe_cols], df_test_enc[ohe_cols], encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5133b41",
   "metadata": {},
   "source": [
    "### Produce the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', sep='\\t')\n",
    "validation = pd.read_csv('validation.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9728c07",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa9e57",
   "metadata": {},
   "source": [
    "The features exstracted are grouped in many different categories. We have individuated three:\n",
    "\n",
    "- **Static Features**: regard page structure, numbers of links and other aspects connected to the page that in general changes very slow\n",
    "- **Semi-Dynamic Features**: regard information about wikipedia network like page references or page links, in general they tend to change in large amount of time\n",
    "- **Dynamic Features**: Regard information about users and number of iteractions with corpus pages, in general they tend to change very quickly change very quicly and be very interesting because they allow us to classify instances based on natural cultural change\n",
    "\n",
    "In the next cell we had selected the best features for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d02896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFdr, chi2, VarianceThreshold, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe997e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "y_train = train[['label']]\n",
    "y_validation = validation[['label']]\n",
    "# Identificators\n",
    "id_train = train[['wiki_name']]\n",
    "id_validation = validation[['wiki_name']]\n",
    "# Numeric features\n",
    "fe_train = train[['languages', 'num_langs', 'reference','n_mod','back_links', 'G_nodes', 'G_num_cliques', 'G_density', 'G_mean_pr','G_num_components', 'n_visits']]\n",
    "fe_validation = validation[['languages', 'num_langs', 'reference','n_mod','back_links', 'G_nodes', 'G_num_cliques', 'G_density', 'G_mean_pr','G_num_components','n_visits']]\n",
    "# String features\n",
    "fe_str_train = train[['category', 'subcategory', 'type']]\n",
    "fe_str_validation = validation[['category', 'subcategory', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feactures_tr = set (SelectFdr(chi2, alpha=0.05).fit(fe_train, y_train).get_feature_names_out())\n",
    "best_features_te  = set (SelectFdr(chi2, alpha=0.05).fit(fe_validation, y_validation).get_feature_names_out())\n",
    "\n",
    "best_fe = best_feactures_tr.intersection(best_features_te)\n",
    "print(f'best features for train set {best_feactures_tr}')\n",
    "print(f'best features for test set  {best_features_te}')\n",
    "\n",
    "print(f'Absolute best features {best_fe}')\n",
    "\n",
    "fe_train = train[list(best_fe)]\n",
    "fe_validation = validation[list(best_fe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# One-hot-encoding on string features #\n",
    "#######################################\n",
    "\n",
    "train_cat, validation_cat, _ =  onehot_encode(fe_str_train, fe_str_validation, ['category'] )\n",
    "train_scat, validation_scat, _ = onehot_encode(fe_str_train, fe_str_validation, ['subcategory'] )\n",
    "train_t, validation_t, _ = onehot_encode(fe_str_train, fe_str_validation, ['type'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_cat.shape)\n",
    "print(validation_scat.shape)\n",
    "print(validation_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([fe_train, train_cat, train_scat, train_t, y_train], axis=1)\n",
    "validation_set = pd.concat([fe_validation, validation_cat, validation_scat, validation_t, y_validation], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a46a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0b35a",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea974e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44b664",
   "metadata": {},
   "source": [
    "### Agnostic-Representative Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train_set.query(\"label == 0 or label == 1\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_tree = ExtraTreesClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation_set.query(\"label == 0 or label == 1\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeccb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ar_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db137e02",
   "metadata": {},
   "source": [
    "### Agnostic-Exclusive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train_set.query(\"label == 0 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f024d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_tree = ExtraTreesClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation_set.query(\"label == 0 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ae_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660edf4",
   "metadata": {},
   "source": [
    "### Representative-Exclusive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset in order to take only two classes and eliminates the labels of the elements\n",
    "d = train_set.query(\"label == 1 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tree = ExtraTreesClassifier().fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation_set.query(\"label == 1 or label == 2\")\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204616b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = re_tree.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255d629",
   "metadata": {},
   "source": [
    "## Voting Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "class CABNet(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Cultural-Classification Network:\n",
    "      - ar: classifier for classes {0,1}\n",
    "      - ae: classifier for classes {0,2}\n",
    "      - re: classifier for classes {1,2}\n",
    "      - detector: optional unsupervised estimator to filter outliers\n",
    "\n",
    "    Combines three binary classifiers with majority voting.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ar_estimator,\n",
    "                 ae_estimator,\n",
    "                 re_estimator,\n",
    "                 th=0.45,\n",
    "                 th_ar=0,\n",
    "                 th_ae=-0.6,\n",
    "                 th_re=0,\n",
    "                 detector=None) -> None:\n",
    "        # Store base estimators for hyperparameter tuning\n",
    "        self.ar_estimator = ar_estimator\n",
    "        self.ae_estimator = ae_estimator\n",
    "        self.re_estimator = re_estimator\n",
    "        self.global_estimator = ExtraTreesClassifier()\n",
    "        self.th = th\n",
    "        self.th_ar = th_ar\n",
    "        self.th_ae = th_ae\n",
    "        self.th_re = th_re\n",
    "        self.detector = detector\n",
    "        \n",
    "        # Label maps\n",
    "        self._label_map = {\n",
    "            'ar': {0, 1},\n",
    "            'ae': {0, 2},\n",
    "            're': {1, 2}\n",
    "        }\n",
    "\n",
    "    def _fit_binary(self, estimator, X, y):\n",
    "        return estimator.fit(X, y)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit CABNet on feature matrix X (array-like, shape (n_samples, n_features))\n",
    "        and labels y (array-like, shape (n_samples,)).\n",
    "        \"\"\"\n",
    "        X_arr = np.asarray(X, dtype=float)\n",
    "        y_arr = np.asarray(y, dtype=int)\n",
    "\n",
    "        # Optional outlier removal via detector\n",
    "        if self.detector is not None:\n",
    "            mask_inliers = self.detector.fit_predict(X_arr) == 1\n",
    "            X_in = X_arr[mask_inliers]\n",
    "            y_in = y_arr[mask_inliers]\n",
    "        else:\n",
    "            X_in = X_arr\n",
    "            y_in = y_arr\n",
    "\n",
    "\n",
    "        # global 3 class estimator\n",
    "        \n",
    "        X_ar, y_ar = X_in, y_in\n",
    "        self.global_estimator = self._fit_binary(self.global_estimator, X_ar, y_ar)\n",
    "\n",
    "        # Train ar: classes 0 vs 1\n",
    "        mask_ar = np.isin(y_in, list(self._label_map['ar']))\n",
    "        X_ar, y_ar = X_in[mask_ar], y_in[mask_ar]\n",
    "        self.ar_estimator = self._fit_binary(self.ar_estimator, X_ar, y_ar)\n",
    "\n",
    "        # Train ae: classes 0 vs 2\n",
    "        mask_ae = np.isin(y_in, list(self._label_map['ae']))\n",
    "        X_ae, y_ae = X_in[mask_ae], y_in[mask_ae]\n",
    "        self.ae_estimator = self._fit_binary(self.ae_estimator, X_ae, y_ae)\n",
    "\n",
    "        # Train re: classes 1 vs 2\n",
    "        mask_re = np.isin(y_in, list(self._label_map['re']))\n",
    "        X_re, y_re = X_in[mask_re], y_in[mask_re]\n",
    "        self.re_estimator = self._fit_binary(self.re_estimator, X_re, y_re)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \"\"\"\n",
    "\n",
    "        threshold = self.th\n",
    "        X_arr = np.asarray(X, dtype=float)\n",
    "        # Individual predictions\n",
    "        v1 = self.ar_estimator.predict(X_arr)\n",
    "        v2 = self.ae_estimator.predict(X_arr)\n",
    "        v3 = self.re_estimator.predict(X_arr)\n",
    "        v4 = self.global_estimator.predict(X_arr)\n",
    "\n",
    "        p1 = self.ar_estimator.predict_proba(X_arr)\n",
    "        p2 = self.ae_estimator.predict_proba(X_arr)\n",
    "        p3 = self.re_estimator.predict_proba(X_arr)\n",
    "        p4 = self.global_estimator.predict_proba(X_arr)\n",
    "\n",
    "        \n",
    "        n = X_arr.shape[0]\n",
    "        final_votes = np.empty(n, dtype=v1.dtype)\n",
    "\n",
    "        for i in range(n):\n",
    "            # raccogli i voti “sicuri”\n",
    "            votes_safe = []\n",
    "            if p1[i].max() >= threshold + self.th_ar:\n",
    "                votes_safe.append(v1[i])\n",
    "            if p2[i].max() >= threshold + self.th_ae:\n",
    "                votes_safe.append(v2[i]) \n",
    "            if p3[i].max() >= threshold + self.th_re:\n",
    "                votes_safe.append(v3[i])\n",
    "            \n",
    "            if votes_safe:\n",
    "                # maggioranza fra i voti sicuri\n",
    "                maj, _ = mode(votes_safe, keepdims=False)\n",
    "                final_votes[i] = maj\n",
    "            else:\n",
    "                probs = [p1[i].max(), p2[i].max(), p3[i].max(), p4[i].max()]\n",
    "                votes = [v1[i],    v2[i],    v3[i],    v4[i]]\n",
    "                best_idx = int(np.argmax(probs))\n",
    "                final_votes[i] = votes[best_idx]\n",
    "    \n",
    "        return final_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'ar_estimator__n_estimators': [50, 100, 170],\n",
    "    'ar_estimator__max_depth': [None, 10],\n",
    "    'ae_estimator__n_estimators': [50, 100, 150],\n",
    "    'ae_estimator__max_depth': [None, 10],\n",
    "    're_estimator__n_estimators': [50, 100, 150],\n",
    "    're_estimator__max_depth': [None, 10],\n",
    "    'detector' : LocalOutlierFactor(),\n",
    "    #'th' : [0.45,0,0.50,.65,.80],\n",
    "    #'th_ar' : [0, +0.05, -0.05, +0.010, -0.010],\n",
    "    #'th_ae' : [0, +0.05, -0.05, +0.010, -0.010],\n",
    "    #'th_re' : [0, +0.05, -0.05, +0.010, -0.010]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    estimator=CABNet(ExtraTreesClassifier(), ExtraTreesClassifier(), ExtraTreesClassifier(), detector=LocalOutlierFactor()),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    cv=2\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_set\n",
    "y = d['label'].astype(int)\n",
    "x = d.drop(['label'], axis=1).astype(float)\n",
    "\n",
    "##############################\n",
    "# For exaustive Gride Search #\n",
    "##############################\n",
    "grid.fit(x,y)\n",
    "print(grid.best_params_, grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3358e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  CABNet(ExtraTreesClassifier(150), ExtraTreesClassifier(100), ExtraTreesClassifier(90), th=0.45,th_ae=-0.05, th_ar=0.00, th_re=0.00, detector=LocalOutlierFactor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_set\n",
    "y = d['label'].astype(int)\n",
    "x = d.drop(['label'], axis=1).astype(float)\n",
    "\n",
    "model = model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = validation_set\n",
    "y = d['label'].astype(int).to_numpy()\n",
    "x = d.drop(['label'], axis=1).astype(float).to_numpy()\n",
    "\n",
    "y_pred = model.predict(x)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Real Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
